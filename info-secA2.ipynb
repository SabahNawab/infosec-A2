{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-05T10:11:07.059215Z",
     "iopub.status.idle": "2025-04-05T10:11:07.059576Z",
     "shell.execute_reply": "2025-04-05T10:11:07.059418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-05T10:11:07.060762Z",
     "iopub.status.idle": "2025-04-05T10:11:07.061125Z",
     "shell.execute_reply": "2025-04-05T10:11:07.060970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "main = pd.read_csv(\"/kaggle/input/infosec-data/FinalDataset/malicious_phish.csv\")\n",
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-05T10:11:07.062020Z",
     "iopub.status.idle": "2025-04-05T10:11:07.062409Z",
     "shell.execute_reply": "2025-04-05T10:11:07.062245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a list of file paths and corresponding types\n",
    "file_paths = [\n",
    "    (\"/kaggle/input/infosec-data/FinalDataset/URL/Benign_list_big_final.csv\", \"benign\"),\n",
    "    (\"/kaggle/input/infosec-data/FinalDataset/URL/DefacementSitesURLFiltered.csv\", \"defacement\"),\n",
    "    (\"/kaggle/input/infosec-data/FinalDataset/URL/Malware_dataset.csv\", \"malware\"),\n",
    "    (\"/kaggle/input/infosec-data/FinalDataset/URL/phishing_dataset.csv\", \"phishing\"),\n",
    "    (\"/kaggle/input/infosec-data/FinalDataset/URL/spam_dataset.csv\", \"spam\")\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Iterate through the file paths and types\n",
    "for file_path, url_type in file_paths:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    # Assign column names\n",
    "    df.columns = [\"url\"]\n",
    "    # Add a column for the URL type\n",
    "    df[\"type\"] = url_type\n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "dfs.append(main)\n",
    "# Concatenate the list of dataframes into a single dataframe\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the merged dataframe\n",
    "merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-05T10:11:07.063777Z",
     "iopub.status.idle": "2025-04-05T10:11:07.064187Z",
     "shell.execute_reply": "2025-04-05T10:11:07.064009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merged_df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T16:12:14.488572Z",
     "iopub.status.busy": "2025-04-04T16:12:14.488310Z",
     "iopub.status.idle": "2025-04-04T16:12:14.552809Z",
     "shell.execute_reply": "2025-04-04T16:12:14.552013Z",
     "shell.execute_reply.started": "2025-04-04T16:12:14.488540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url     0\n",
       "type    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T16:12:14.553785Z",
     "iopub.status.busy": "2025-04-04T16:12:14.553589Z",
     "iopub.status.idle": "2025-04-04T16:12:14.567151Z",
     "shell.execute_reply": "2025-04-04T16:12:14.566202Z",
     "shell.execute_reply.started": "2025-04-04T16:12:14.553756Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://1337x.to/torrent/1048648/American-Snipe...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://1337x.to/torrent/1110018/Blackhat-2015-...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://1337x.to/torrent/1122940/Blackhat-2015-...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://1337x.to/torrent/1124395/Fast-and-Furio...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://1337x.to/torrent/1145504/Avengers-Age-o...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url    type\n",
       "0  http://1337x.to/torrent/1048648/American-Snipe...  benign\n",
       "1  http://1337x.to/torrent/1110018/Blackhat-2015-...  benign\n",
       "2  http://1337x.to/torrent/1122940/Blackhat-2015-...  benign\n",
       "3  http://1337x.to/torrent/1124395/Fast-and-Furio...  benign\n",
       "4  http://1337x.to/torrent/1145504/Avengers-Age-o...  benign"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T16:12:14.568226Z",
     "iopub.status.busy": "2025-04-04T16:12:14.567995Z",
     "iopub.status.idle": "2025-04-04T16:12:14.582075Z",
     "shell.execute_reply": "2025-04-04T16:12:14.581190Z",
     "shell.execute_reply.started": "2025-04-04T16:12:14.568208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    http://1337x.to/torrent/1048648/American-Snipe...\n",
       "1    http://1337x.to/torrent/1110018/Blackhat-2015-...\n",
       "2    http://1337x.to/torrent/1122940/Blackhat-2015-...\n",
       "3    http://1337x.to/torrent/1124395/Fast-and-Furio...\n",
       "4    http://1337x.to/torrent/1145504/Avengers-Age-o...\n",
       "Name: url, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(5)['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-05T10:11:07.065223Z",
     "iopub.status.idle": "2025-04-05T10:11:07.065582Z",
     "shell.execute_reply": "2025-04-05T10:11:07.065426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-05T10:11:07.066966Z",
     "iopub.status.idle": "2025-04-05T10:11:07.067341Z",
     "shell.execute_reply": "2025-04-05T10:11:07.067181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "df=merged_df.copy()\n",
    "\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['type_encoded'] = label_encoder.fit_transform(df['type'])\n",
    "\n",
    "\n",
    "print(\"Class Mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "\n",
    "df_majority = df[df['type'] == 'benign']\n",
    "df_minority = df[df['type'] != 'benign']\n",
    "\n",
    "\n",
    "majority_count = len(df_majority)\n",
    "\n",
    "df_balanced = df_majority.copy()\n",
    "for class_label in df['type'].unique():\n",
    "    if class_label != 'benign':  \n",
    "        df_minority_class = df[df['type'] == class_label]\n",
    "        df_minority_upsampled = resample(df_minority_class,\n",
    "                                         replace=True,  \n",
    "                                         n_samples=majority_count,  \n",
    "                                         random_state=42)\n",
    "        df_balanced = pd.concat([df_balanced, df_minority_upsampled])\n",
    "\n",
    "\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=df_balanced['type'], order=df_balanced['type'].value_counts().index, palette=\"viridis\")\n",
    "plt.title(\"Balanced Distribution of URL Types\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(df_balanced.describe(include='all'))\n",
    "\n",
    "# Further EDA (Explore URL structures, tokenization, etc.)\n",
    "df_balanced['url_length'] = df_balanced['url'].apply(len)  # Example feature: URL length\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=df_balanced['type'], y=df_balanced['url_length'], palette=\"coolwarm\")\n",
    "plt.title(\"URL Length Distribution Across Classes\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Print balanced dataset distribution\n",
    "print(\"Class Distribution After Oversampling:\", Counter(df_balanced['type']))\n",
    "\n",
    "# Save the processed data\n",
    "df_balanced.to_csv(\"processed_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-05T10:11:07.068376Z",
     "iopub.status.idle": "2025-04-05T10:11:07.068763Z",
     "shell.execute_reply": "2025-04-05T10:11:07.068579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-05T10:11:07.069579Z",
     "iopub.status.idle": "2025-04-05T10:11:07.069880Z",
     "shell.execute_reply": "2025-04-05T10:11:07.069757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 1️⃣ Special character frequency in URLs\n",
    "special_chars = ['@', '-', '_', '=', '?', '&', '%', '.']\n",
    "\n",
    "# Escape special characters before counting\n",
    "char_counts = {char: df_balanced['url'].str.count(re.escape(char)).sum() for char in special_chars}\n",
    "\n",
    "# Plot the special character frequencies\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=list(char_counts.keys()), y=list(char_counts.values()), palette=\"coolwarm\")\n",
    "plt.title(\"Frequency of Special Characters in URLs\")\n",
    "plt.xlabel(\"Special Characters\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2️⃣ Top subdomains used in malicious URLs\n",
    "df_balanced['subdomain'] = df_balanced['url'].str.extract(r'://([a-zA-Z0-9.-]+)\\.')\n",
    "top_subdomains = df_balanced['subdomain'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=top_subdomains.index, y=top_subdomains.values, palette=\"viridis\")\n",
    "plt.title(\"Top 10 Subdomains in URLs\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 3️⃣ WordCloud for URL words\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = ' '.join(df_balanced['url'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='black').generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud of URL Words\")\n",
    "plt.show()\n",
    "\n",
    "# 4️⃣ Distribution of URL lengths per category\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data=df_balanced, x='url_length', hue='type', bins=30, kde=True, palette=\"Set1\")\n",
    "plt.title(\"URL Length Distribution by Category\")\n",
    "plt.xlabel(\"URL Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 5️⃣ Top-Level Domain (TLD) distribution\n",
    "df_balanced['tld'] = df_balanced['url'].str.extract(r'\\.([a-z]+)$')\n",
    "top_tlds = df_balanced['tld'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=top_tlds.index, y=top_tlds.values, palette=\"magma\")\n",
    "plt.title(\"Top 10 TLDs in Malicious and Benign URLs\")\n",
    "plt.xlabel(\"Top-Level Domain (TLD)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:36:09.108453Z",
     "iopub.status.busy": "2025-04-03T18:36:09.107976Z",
     "iopub.status.idle": "2025-04-03T18:36:09.113635Z",
     "shell.execute_reply": "2025-04-03T18:36:09.112877Z",
     "shell.execute_reply.started": "2025-04-03T18:36:09.108427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653046"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T11:25:27.844242Z",
     "iopub.status.busy": "2025-04-04T11:25:27.843915Z",
     "iopub.status.idle": "2025-04-04T11:40:25.052000Z",
     "shell.execute_reply": "2025-04-04T11:40:25.051313Z",
     "shell.execute_reply.started": "2025-04-04T11:25:27.844217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 80ms/step - accuracy: 0.9000 - loss: 0.2582\n",
      "Epoch 2/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 80ms/step - accuracy: 0.9533 - loss: 0.1093\n",
      "Epoch 3/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 81ms/step - accuracy: 0.9568 - loss: 0.0987\n",
      "Epoch 4/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 81ms/step - accuracy: 0.9589 - loss: 0.0929\n",
      "Epoch 5/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 80ms/step - accuracy: 0.9603 - loss: 0.0890\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tldextract\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# Load Data in Batches\n",
    "CHUNK_SIZE = 100000  # Load data in chunks to prevent memory overflow\n",
    "df_chunks = pd.read_csv(\"/kaggle/working/processed_dataset.csv\", chunksize=CHUNK_SIZE)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# TF-IDF Vectorizer (Fitted on a subset to save memory)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "# Read a small sample for TF-IDF fitting\n",
    "small_sample = pd.read_csv(\"/kaggle/working/processed_dataset.csv\", nrows=50000)  \n",
    "tfidf_vectorizer.fit(small_sample[\"url\"])\n",
    "\n",
    "\n",
    "# Define Feature Extraction Function\n",
    "def extract_features(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return {\n",
    "        \"url_length\": len(url),\n",
    "        \"num_digits\": sum(c.isdigit() for c in url),\n",
    "        \"num_special_chars\": sum(c in \"!@#$%^&*()_+=\" for c in url),\n",
    "        \"num_subdomains\": len(ext.subdomain.split(\".\")),\n",
    "        \"has_https\": 1 if url.startswith(\"https\") else 0\n",
    "    }\n",
    "\n",
    "# Model Initialization\n",
    "sgd_model = SGDClassifier(loss=\"log_loss\", learning_rate=\"optimal\", max_iter=1000, tol=1e-3)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100)\n",
    "\n",
    "# Batch Processing\n",
    "for df in df_chunks:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Encode Labels\n",
    "    if \"type_encoded\" not in df:\n",
    "        df[\"type_encoded\"] = label_encoder.fit_transform(df[\"type\"])\n",
    "\n",
    "    # Feature Extraction\n",
    "    features_df = df[\"url\"].apply(lambda x: pd.Series(extract_features(x)))\n",
    "    df = pd.concat([df, features_df], axis=1)\n",
    "\n",
    "    # TF-IDF Transformation in Batches\n",
    "    X_tfidf = tfidf_vectorizer.transform(df[\"url\"])\n",
    "\n",
    "    # Structural Features\n",
    "    X_structural = df[[\"url_length\", \"num_digits\", \"num_special_chars\", \"num_subdomains\", \"has_https\"]].values\n",
    "\n",
    "    # Combine Features\n",
    "    X = np.hstack((X_tfidf.toarray(), X_structural))\n",
    "    y = df[\"type_encoded\"]\n",
    "\n",
    "    # Balance Dataset Per Batch\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Train Models (Incremental Learning)\n",
    "    sgd_model.partial_fit(X_resampled, y_resampled, classes=np.unique(y))\n",
    "    xgb_model.fit(X_resampled, y_resampled)\n",
    "    lgb_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Free Memory\n",
    "    del df, X, y, X_resampled, y_resampled\n",
    "    gc.collect()\n",
    "\n",
    "# Save Models\n",
    "joblib.dump(sgd_model, \"sgd_model.pkl\")\n",
    "joblib.dump(xgb_model, \"xgb_model.pkl\")\n",
    "joblib.dump(lgb_model, \"lgb_model.pkl\")\n",
    "\n",
    "\n",
    "class URLSequence(Sequence):\n",
    "    def __init__(self, file_path, batch_size=1024, max_length=50, num_words=5000):\n",
    "        super().__init__()  # ✅ Correct location\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = Tokenizer(num_words=num_words)\n",
    "        self.max_length = max_length\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # Fit tokenizer on small data subset\n",
    "        small_sample = pd.read_csv(file_path, nrows=50000)\n",
    "        self.tokenizer.fit_on_texts(small_sample[\"url\"])\n",
    "\n",
    "        # Label encoder setup\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(small_sample[\"type\"])\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "\n",
    "        self.reset_iterator()\n",
    "\n",
    "    def reset_iterator(self):\n",
    "        self.data_chunks = pd.read_csv(self.file_path, chunksize=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(1 for _ in pd.read_csv(self.file_path, chunksize=self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            df = next(self.data_chunks)\n",
    "        except StopIteration:\n",
    "            self.reset_iterator()\n",
    "            df = next(self.data_chunks)\n",
    "\n",
    "        X_seq = pad_sequences(self.tokenizer.texts_to_sequences(df[\"url\"]), maxlen=self.max_length)\n",
    "        y_seq = self.label_encoder.transform(df[\"type\"])\n",
    "        return X_seq, y_seq\n",
    "\n",
    "\n",
    "# Config\n",
    "num_words = 5000\n",
    "batch_size = 1024\n",
    "\n",
    "# Label encoding on full label space to get class count\n",
    "sample_data = pd.read_csv(\"/kaggle/working/processed_dataset.csv\", nrows=50000)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(sample_data[\"type\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Generator\n",
    "train_gen = URLSequence(\"/kaggle/working/processed_dataset.csv\", batch_size=batch_size, num_words=num_words)\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=128),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "lstm_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "lstm_model.fit(train_gen, epochs=5)\n",
    "\n",
    "# Save\n",
    "lstm_model.save(\"lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:40:47.518260Z",
     "iopub.status.busy": "2025-04-03T18:40:47.517915Z",
     "iopub.status.idle": "2025-04-03T18:40:47.523499Z",
     "shell.execute_reply": "2025-04-03T18:40:47.522703Z",
     "shell.execute_reply.started": "2025-04-03T18:40:47.518232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.parsers.readers.TextFileReader at 0x7edfacbb39d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:03:09.932864Z",
     "iopub.status.busy": "2025-04-04T12:03:09.932490Z",
     "iopub.status.idle": "2025-04-04T15:55:05.971632Z",
     "shell.execute_reply": "2025-04-04T15:55:05.970518Z",
     "shell.execute_reply.started": "2025-04-04T12:03:09.932835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.755758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116297\n",
      "[LightGBM] [Info] Number of data points in the train set: 99325, number of used features: 2984\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.767031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115466\n",
      "[LightGBM] [Info] Number of data points in the train set: 99440, number of used features: 2929\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.769730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 117736\n",
      "[LightGBM] [Info] Number of data points in the train set: 100930, number of used features: 2985\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.766216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116583\n",
      "[LightGBM] [Info] Number of data points in the train set: 99750, number of used features: 2935\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.768454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 114948\n",
      "[LightGBM] [Info] Number of data points in the train set: 99535, number of used features: 2905\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.796918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116823\n",
      "[LightGBM] [Info] Number of data points in the train set: 100455, number of used features: 2965\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.775590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115542\n",
      "[LightGBM] [Info] Number of data points in the train set: 99390, number of used features: 2893\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.794288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 118543\n",
      "[LightGBM] [Info] Number of data points in the train set: 100635, number of used features: 2985\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.766587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115593\n",
      "[LightGBM] [Info] Number of data points in the train set: 99585, number of used features: 2882\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.771310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117680\n",
      "[LightGBM] [Info] Number of data points in the train set: 100590, number of used features: 2962\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.782871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116737\n",
      "[LightGBM] [Info] Number of data points in the train set: 100030, number of used features: 2956\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.815757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 117660\n",
      "[LightGBM] [Info] Number of data points in the train set: 101025, number of used features: 3014\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.789335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116010\n",
      "[LightGBM] [Info] Number of data points in the train set: 99435, number of used features: 2941\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.750705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116585\n",
      "[LightGBM] [Info] Number of data points in the train set: 100415, number of used features: 2907\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.776252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116847\n",
      "[LightGBM] [Info] Number of data points in the train set: 100035, number of used features: 2942\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.806670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 118460\n",
      "[LightGBM] [Info] Number of data points in the train set: 100965, number of used features: 2999\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.775890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116374\n",
      "[LightGBM] [Info] Number of data points in the train set: 99765, number of used features: 2917\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.776095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116172\n",
      "[LightGBM] [Info] Number of data points in the train set: 99635, number of used features: 2931\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.772944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115740\n",
      "[LightGBM] [Info] Number of data points in the train set: 99370, number of used features: 2931\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.745823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116198\n",
      "[LightGBM] [Info] Number of data points in the train set: 100105, number of used features: 2910\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.787577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116432\n",
      "[LightGBM] [Info] Number of data points in the train set: 99710, number of used features: 2975\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 48129\n",
      "[LightGBM] [Info] Number of data points in the train set: 40275, number of used features: 1370\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Epoch 1/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 80ms/step - accuracy: 0.9021 - loss: 0.2556\n",
      "Epoch 2/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 80ms/step - accuracy: 0.9532 - loss: 0.1097\n",
      "Epoch 3/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 81ms/step - accuracy: 0.9568 - loss: 0.0990\n",
      "Epoch 4/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 80ms/step - accuracy: 0.9590 - loss: 0.0932\n",
      "Epoch 5/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 80ms/step - accuracy: 0.9604 - loss: 0.0892\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-be2834818d7b>\u001b[0m in \u001b[0;36m<cell line: 162>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xgb_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0mlgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lgb_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;31m# Split Data for Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tldextract\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Data in Batches\n",
    "CHUNK_SIZE = 100000  # Load data in chunks to prevent memory overflow\n",
    "df_chunks = pd.read_csv(\"/kaggle/working/processed_dataset.csv\", chunksize=CHUNK_SIZE)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# TF-IDF Vectorizer (Fitted on a subset to save memory)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "# Read a small sample for TF-IDF fitting\n",
    "small_sample = pd.read_csv(\"/kaggle/working/processed_dataset.csv\", nrows=50000)\n",
    "tfidf_vectorizer.fit(small_sample[\"url\"])\n",
    "\n",
    "# Define Feature Extraction Function\n",
    "def extract_features(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return {\n",
    "        \"url_length\": len(url),\n",
    "        \"num_digits\": sum(c.isdigit() for c in url),\n",
    "        \"num_special_chars\": sum(c in \"!@#$%^&*()_+=\" for c in url),\n",
    "        \"num_subdomains\": len(ext.subdomain.split(\".\")),\n",
    "        \"has_https\": 1 if url.startswith(\"https\") else 0\n",
    "    }\n",
    "\n",
    "# Model Initialization\n",
    "sgd_model = SGDClassifier(loss=\"log_loss\", learning_rate=\"optimal\", max_iter=1000, tol=1e-3)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100)\n",
    "\n",
    "# Batch Processing\n",
    "for df in df_chunks:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Encode Labels\n",
    "    if \"type_encoded\" not in df:\n",
    "        df[\"type_encoded\"] = label_encoder.fit_transform(df[\"type\"])\n",
    "\n",
    "    # Feature Extraction\n",
    "    features_df = df[\"url\"].apply(lambda x: pd.Series(extract_features(x)))\n",
    "    df = pd.concat([df, features_df], axis=1)\n",
    "\n",
    "    # TF-IDF Transformation in Batches\n",
    "    X_tfidf = tfidf_vectorizer.transform(df[\"url\"])\n",
    "\n",
    "    # Structural Features\n",
    "    X_structural = df[[\"url_length\", \"num_digits\", \"num_special_chars\", \"num_subdomains\", \"has_https\"]].values\n",
    "\n",
    "    # Combine Features\n",
    "    X = np.hstack((X_tfidf.toarray(), X_structural))\n",
    "    y = df[\"type_encoded\"]\n",
    "\n",
    "    # Balance Dataset Per Batch\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Train Models (Incremental Learning)\n",
    "    sgd_model.partial_fit(X_resampled, y_resampled, classes=np.unique(y))\n",
    "    xgb_model.fit(X_resampled, y_resampled)\n",
    "    lgb_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Free Memory\n",
    "    del df, X, y, X_resampled, y_resampled\n",
    "    gc.collect()\n",
    "\n",
    "# Save Models\n",
    "joblib.dump(sgd_model, \"sgd_model.pkl\")\n",
    "joblib.dump(xgb_model, \"xgb_model.pkl\")\n",
    "joblib.dump(lgb_model, \"lgb_model.pkl\")\n",
    "\n",
    "\n",
    "class URLSequence(Sequence):\n",
    "    def __init__(self, file_path, batch_size=1024, max_length=50, num_words=5000):\n",
    "        super().__init__()  # ✅ Correct location\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = Tokenizer(num_words=num_words)\n",
    "        self.max_length = max_length\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # Fit tokenizer on small data subset\n",
    "        small_sample = pd.read_csv(file_path, nrows=50000)\n",
    "        self.tokenizer.fit_on_texts(small_sample[\"url\"])\n",
    "\n",
    "        # Label encoder setup\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(small_sample[\"type\"])\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "\n",
    "        self.reset_iterator()\n",
    "\n",
    "    def reset_iterator(self):\n",
    "        self.data_chunks = pd.read_csv(self.file_path, chunksize=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(1 for _ in pd.read_csv(self.file_path, chunksize=self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            df = next(self.data_chunks)\n",
    "        except StopIteration:\n",
    "            self.reset_iterator()\n",
    "            df = next(self.data_chunks)\n",
    "\n",
    "        X_seq = pad_sequences(self.tokenizer.texts_to_sequences(df[\"url\"]), maxlen=self.max_length)\n",
    "        y_seq = self.label_encoder.transform(df[\"type\"])\n",
    "        return X_seq, y_seq\n",
    "\n",
    "\n",
    "# Config\n",
    "num_words = 5000\n",
    "batch_size = 1024\n",
    "\n",
    "# Label encoding on full label space to get class count\n",
    "sample_data = pd.read_csv(\"/kaggle/working/processed_dataset.csv\", nrows=50000)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(sample_data[\"type\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Generator\n",
    "train_gen = URLSequence(\"/kaggle/working/processed_dataset.csv\", batch_size=batch_size, num_words=num_words)\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=128),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "lstm_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "lstm_model.fit(train_gen, epochs=5)\n",
    "\n",
    "# Save LSTM Model\n",
    "lstm_model.save(\"lstm_model.h5\")\n",
    "\n",
    "# Load Models for Evaluation\n",
    "sgd_model = joblib.load(\"sgd_model.pkl\")\n",
    "xgb_model = joblib.load(\"xgb_model.pkl\")\n",
    "lgb_model = joblib.load(\"lgb_model.pkl\")\n",
    "lstm_model = load_model(\"lstm_model.h5\")\n",
    "\n",
    "# Split Data for Evaluation\n",
    "data = pd.read_csv(\"/kaggle/working/processed_dataset.csv\", nrows=100000)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)  # Split test into val & test\n",
    "\n",
    "# Encode Labels\n",
    "y_train = label_encoder.fit_transform(train_data[\"type\"])\n",
    "y_val = label_encoder.transform(val_data[\"type\"])\n",
    "y_test = label_encoder.transform(test_data[\"type\"])\n",
    "\n",
    "# TF-IDF Transformation\n",
    "X_train_tfidf = tfidf_vectorizer.transform(train_data[\"url\"])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_data[\"url\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data[\"url\"])\n",
    "\n",
    "# Extract Structural Features\n",
    "def extract_features(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return [\n",
    "        len(url),\n",
    "        sum(c.isdigit() for c in url),\n",
    "        sum(c in \"!@#$%^&*()_+=\" for c in url),\n",
    "        len(ext.subdomain.split(\".\")),\n",
    "        1 if url.startswith(\"https\") else 0\n",
    "    ]\n",
    "\n",
    "train_struct = np.array([extract_features(u) for u in train_data[\"url\"]])\n",
    "val_struct = np.array([extract_features(u) for u in val_data[\"url\"]])\n",
    "test_struct = np.array([extract_features(u) for u in test_data[\"url\"]])\n",
    "\n",
    "X_train_comb = np.hstack((X_train_tfidf.toarray(), train_struct))\n",
    "X_val_comb = np.hstack((X_val_tfidf.toarray(), val_struct))\n",
    "X_test_comb = np.hstack((X_test_tfidf.toarray(), test_struct))\n",
    "\n",
    "print(\"🔍 SGD Validation Accuracy:\", accuracy_score(y_val, sgd_model.predict(X_val_comb)))\n",
    "print(\"✅ SGD Test Accuracy:\", accuracy_score(y_test, sgd_model.predict(X_test_comb)))\n",
    "\n",
    "print(\"🔍 XGBoost Validation Accuracy:\", accuracy_score(y_val, xgb_model.predict(X_val_comb)))\n",
    "print(\"✅ XGBoost Test Accuracy:\", accuracy_score(y_test, xgb_model.predict(X_test_comb)))\n",
    "\n",
    "print(\"🔍 LightGBM Validation Accuracy:\", accuracy_score(y_val, lgb_model.predict(X_val_comb)))\n",
    "print(\"✅ LightGBM Test Accuracy:\", accuracy_score(y_test, lgb_model.predict(X_test_comb)))\n",
    "\n",
    "\n",
    "X_val_seq = pad_sequences(tokenizer.texts_to_sequences(val_data[\"url\"]), maxlen=50)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_data[\"url\"]), maxlen=50)\n",
    "\n",
    "val_loss, val_acc = lstm_model.evaluate(X_val_seq, y_val, verbose=0)\n",
    "test_loss, test_acc = lstm_model.evaluate(X_test_seq, y_test, verbose=0)\n",
    "\n",
    "print(\"🔍 LSTM Validation Accuracy:\", round(val_acc, 4))\n",
    "print(\"✅ LSTM Test Accuracy:\", round(test_acc, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T10:12:35.282081Z",
     "iopub.status.busy": "2025-04-05T10:12:35.281818Z",
     "iopub.status.idle": "2025-04-05T13:57:03.884890Z",
     "shell.execute_reply": "2025-04-05T13:57:03.884132Z",
     "shell.execute_reply.started": "2025-04-05T10:12:35.282053Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.831920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 115406\n",
      "[LightGBM] [Info] Number of data points in the train set: 99325, number of used features: 2974\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.843599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 114903\n",
      "[LightGBM] [Info] Number of data points in the train set: 99440, number of used features: 2924\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.962835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 116684\n",
      "[LightGBM] [Info] Number of data points in the train set: 100930, number of used features: 2966\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.896441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115492\n",
      "[LightGBM] [Info] Number of data points in the train set: 99750, number of used features: 2905\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.979178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 114141\n",
      "[LightGBM] [Info] Number of data points in the train set: 99535, number of used features: 2893\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.927244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115907\n",
      "[LightGBM] [Info] Number of data points in the train set: 100455, number of used features: 2952\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.897115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 114657\n",
      "[LightGBM] [Info] Number of data points in the train set: 99390, number of used features: 2893\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.023595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117439\n",
      "[LightGBM] [Info] Number of data points in the train set: 100635, number of used features: 2963\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.864435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 114702\n",
      "[LightGBM] [Info] Number of data points in the train set: 99585, number of used features: 2875\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.982748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116683\n",
      "[LightGBM] [Info] Number of data points in the train set: 100590, number of used features: 2944\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.923914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115713\n",
      "[LightGBM] [Info] Number of data points in the train set: 100030, number of used features: 2936\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.976216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116709\n",
      "[LightGBM] [Info] Number of data points in the train set: 101025, number of used features: 2996\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.919114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115137\n",
      "[LightGBM] [Info] Number of data points in the train set: 99435, number of used features: 2938\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.945651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115789\n",
      "[LightGBM] [Info] Number of data points in the train set: 100415, number of used features: 2909\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.161751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116078\n",
      "[LightGBM] [Info] Number of data points in the train set: 100035, number of used features: 2943\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.995221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117273\n",
      "[LightGBM] [Info] Number of data points in the train set: 100965, number of used features: 2975\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.880547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115429\n",
      "[LightGBM] [Info] Number of data points in the train set: 99765, number of used features: 2900\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.953641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 115133\n",
      "[LightGBM] [Info] Number of data points in the train set: 99635, number of used features: 2919\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.928502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 114979\n",
      "[LightGBM] [Info] Number of data points in the train set: 99370, number of used features: 2920\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.911433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 115266\n",
      "[LightGBM] [Info] Number of data points in the train set: 100105, number of used features: 2896\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.973954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 115454\n",
      "[LightGBM] [Info] Number of data points in the train set: 99710, number of used features: 2962\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47691\n",
      "[LightGBM] [Info] Number of data points in the train set: 40275, number of used features: 1362\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "Epoch 1/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 80ms/step - accuracy: 0.8997 - loss: 0.2591\n",
      "Epoch 2/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 80ms/step - accuracy: 0.9534 - loss: 0.1089\n",
      "Epoch 3/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 81ms/step - accuracy: 0.9568 - loss: 0.0988\n",
      "Epoch 4/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 81ms/step - accuracy: 0.9589 - loss: 0.0932\n",
      "Epoch 5/5\n",
      "\u001b[1m2091/2091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 82ms/step - accuracy: 0.9602 - loss: 0.0894\n",
      "🔍 SGD Validation Accuracy: 0.5885\n",
      "✅ SGD Test Accuracy: 0.595\n",
      "🔍 XGBoost Validation Accuracy: 0.9295\n",
      "✅ XGBoost Test Accuracy: 0.9327\n",
      "🔍 LightGBM Validation Accuracy: 0.9333\n",
      "✅ LightGBM Test Accuracy: 0.9347\n",
      "🔍 LSTM Validation Accuracy: 0.9621\n",
      "✅ LSTM Test Accuracy: 0.9609\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tldextract\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Paths and config\n",
    "DATA_PATH = \"/kaggle/working/processed_dataset.csv\"\n",
    "CHUNK_SIZE = 100000\n",
    "MAX_FEATURES = 5000\n",
    "STRUCTURAL_FEATURES = 10\n",
    "\n",
    "# Define Feature Extraction Function\n",
    "def extract_features(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return [\n",
    "        len(url),\n",
    "        sum(c.isdigit() for c in url),\n",
    "        sum(c in \"!@#$%^&*()_+=\" for c in url),\n",
    "        len(ext.subdomain.split(\".\")),\n",
    "        1 if url.startswith(\"https\") else 0\n",
    "    ]\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=MAX_FEATURES, stop_words=\"english\")\n",
    "small_sample = pd.read_csv(DATA_PATH, nrows=50000)\n",
    "tfidf_vectorizer.fit(small_sample[\"url\"])\n",
    "\n",
    "# Label encoding setup\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(small_sample[\"type\"])\n",
    "\n",
    "# Models\n",
    "sgd_model = SGDClassifier(loss=\"log_loss\", learning_rate=\"optimal\", max_iter=1000, tol=1e-3)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100)\n",
    "\n",
    "# Batch Processing and Training\n",
    "df_chunks = pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE)\n",
    "\n",
    "for df in df_chunks:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df[\"type_encoded\"] = label_encoder.transform(df[\"type\"])\n",
    "\n",
    "    # TF-IDF + Structural Features\n",
    "    X_tfidf = tfidf_vectorizer.transform(df[\"url\"])\n",
    "    X_struct = np.array([extract_features(url) for url in df[\"url\"]])\n",
    "    X = np.hstack((X_tfidf.toarray(), X_struct))\n",
    "    y = df[\"type_encoded\"]\n",
    "\n",
    "    # Balance dataset\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "    # Train\n",
    "    sgd_model.partial_fit(X_res, y_res, classes=np.unique(y))\n",
    "    xgb_model.fit(X_res, y_res)\n",
    "    lgb_model.fit(X_res, y_res)\n",
    "\n",
    "    del df, X, y, X_res, y_res\n",
    "    gc.collect()\n",
    "\n",
    "# Save models\n",
    "joblib.dump(sgd_model, \"sgd_model.pkl\")\n",
    "joblib.dump(xgb_model, \"xgb_model.pkl\")\n",
    "joblib.dump(lgb_model, \"lgb_model.pkl\")\n",
    "\n",
    "# LSTM Sequence Loader\n",
    "class URLSequence(Sequence):\n",
    "    def __init__(self, file_path, batch_size=1024, max_length=50, num_words=5000):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.num_words = num_words\n",
    "\n",
    "        small_sample = pd.read_csv(file_path, nrows=50000)\n",
    "        self.tokenizer = Tokenizer(num_words=num_words)\n",
    "        self.tokenizer.fit_on_texts(small_sample[\"url\"])\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(small_sample[\"type\"])\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "\n",
    "        self.reset_iterator()\n",
    "\n",
    "    def reset_iterator(self):\n",
    "        self.data_chunks = pd.read_csv(self.file_path, chunksize=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(1 for _ in pd.read_csv(self.file_path, chunksize=self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            df = next(self.data_chunks)\n",
    "        except StopIteration:\n",
    "            self.reset_iterator()\n",
    "            df = next(self.data_chunks)\n",
    "\n",
    "        X_seq = pad_sequences(self.tokenizer.texts_to_sequences(df[\"url\"]), maxlen=self.max_length)\n",
    "        y_seq = self.label_encoder.transform(df[\"type\"])\n",
    "        return X_seq, y_seq\n",
    "\n",
    "# LSTM Model\n",
    "train_gen = URLSequence(DATA_PATH)\n",
    "tokenizer = train_gen.tokenizer  # ✅ Save tokenizer for evaluation\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=MAX_FEATURES, output_dim=128),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(train_gen.num_classes, activation=\"softmax\")\n",
    "])\n",
    "lstm_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "lstm_model.fit(train_gen, epochs=5)\n",
    "lstm_model.save(\"lstm_model.h5\")\n",
    "\n",
    "# Evaluation\n",
    "sgd_model = joblib.load(\"sgd_model.pkl\")\n",
    "xgb_model = joblib.load(\"xgb_model.pkl\")\n",
    "lgb_model = joblib.load(\"lgb_model.pkl\")\n",
    "lstm_model = load_model(\"lstm_model.h5\")\n",
    "\n",
    "# Test split\n",
    "df_eval = pd.read_csv(DATA_PATH, nrows=100000)\n",
    "train_df, test_df = train_test_split(df_eval, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "y_val = label_encoder.transform(val_df[\"type\"])\n",
    "y_test = label_encoder.transform(test_df[\"type\"])\n",
    "\n",
    "# TF-IDF + Structural for traditional models\n",
    "def prepare_features(df):\n",
    "    tfidf = tfidf_vectorizer.transform(df[\"url\"])\n",
    "    struct = np.array([extract_features(url) for url in df[\"url\"]])\n",
    "    return np.hstack((tfidf.toarray(), struct))\n",
    "\n",
    "X_val = prepare_features(val_df)\n",
    "X_test = prepare_features(test_df)\n",
    "\n",
    "print(\"🔍 SGD Validation Accuracy:\", accuracy_score(y_val, sgd_model.predict(X_val)))\n",
    "print(\"✅ SGD Test Accuracy:\", accuracy_score(y_test, sgd_model.predict(X_test)))\n",
    "\n",
    "print(\"🔍 XGBoost Validation Accuracy:\", accuracy_score(y_val, xgb_model.predict(X_val)))\n",
    "print(\"✅ XGBoost Test Accuracy:\", accuracy_score(y_test, xgb_model.predict(X_test)))\n",
    "\n",
    "print(\"🔍 LightGBM Validation Accuracy:\", accuracy_score(y_val, lgb_model.predict(X_val)))\n",
    "print(\"✅ LightGBM Test Accuracy:\", accuracy_score(y_test, lgb_model.predict(X_test)))\n",
    "\n",
    "# LSTM Eval\n",
    "X_val_seq = pad_sequences(tokenizer.texts_to_sequences(val_df[\"url\"]), maxlen=50)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df[\"url\"]), maxlen=50)\n",
    "\n",
    "val_loss, val_acc = lstm_model.evaluate(X_val_seq, y_val, verbose=0)\n",
    "test_loss, test_acc = lstm_model.evaluate(X_test_seq, y_test, verbose=0)\n",
    "\n",
    "print(\"🔍 LSTM Validation Accuracy:\", round(val_acc, 4))\n",
    "print(\"✅ LSTM Test Accuracy:\", round(test_acc, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T10:12:30.469424Z",
     "iopub.status.busy": "2025-04-05T10:12:30.469125Z",
     "iopub.status.idle": "2025-04-05T10:12:35.280413Z",
     "shell.execute_reply": "2025-04-05T10:12:35.279529Z",
     "shell.execute_reply.started": "2025-04-05T10:12:30.469398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tldextract\n",
      "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.32.3)\n",
      "Collecting requests-file>=1.4 (from tldextract)\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2025.1.31)\n",
      "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: requests-file, tldextract\n",
      "Successfully installed requests-file-2.1.0 tldextract-5.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Model names and their corresponding validation and test accuracies\n",
    "models = ['SGD', 'XGBoost', 'LightGBM', 'LSTM']\n",
    "val_accuracies = [0.5885, 0.9295, 0.9333, 0.9621]\n",
    "test_accuracies = [0.595, 0.9327, 0.9347, 0.9609]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, val_accuracies, width, label='Validation Accuracy', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, test_accuracies, width, label='Test Accuracy', color='lightgreen')\n",
    "\n",
    "# Add labels on top\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Accuracy Comparison (Validation vs Test)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# Simulated settings\n",
    "n_samples = 500\n",
    "n_classes = 4\n",
    "class_names = ['benign', 'phishing', 'malware', 'defacement']\n",
    "accuracies = {\n",
    "    \"SGD\": 0.595,\n",
    "    \"XGBoost\": 0.9327,\n",
    "    \"LightGBM\": 0.9347,\n",
    "    \"LSTM\": 0.9609\n",
    "}\n",
    "\n",
    "# Simulate true labels\n",
    "np.random.seed(42)\n",
    "y_test_sim = np.random.randint(0, n_classes, size=n_samples)\n",
    "\n",
    "def simulate_confusion_matrix(y_true, accuracy, n_classes):\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for label in range(n_classes):\n",
    "        class_indices = np.where(y_true == label)[0]\n",
    "        n_class_samples = len(class_indices)\n",
    "        n_correct = int(n_class_samples * accuracy)\n",
    "        n_incorrect = n_class_samples - n_correct\n",
    "\n",
    "        # Add correct predictions\n",
    "        cm[label][label] += n_correct\n",
    "\n",
    "        # Misclassify the rest randomly to other classes\n",
    "        if n_incorrect > 0:\n",
    "            other_classes = [i for i in range(n_classes) if i != label]\n",
    "            mislabels = np.random.choice(other_classes, n_incorrect)\n",
    "            for pred in mislabels:\n",
    "                cm[label][pred] += 1\n",
    "    return cm\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model, acc) in enumerate(accuracies.items()):\n",
    "    cm = simulate_confusion_matrix(y_test_sim, acc, n_classes)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=axes[i])\n",
    "    axes[i].set_title(f\"{model} (Accuracy ≈ {acc:.2f})\")\n",
    "    axes[i].set_xlabel(\"Predicted Label\")\n",
    "    axes[i].set_ylabel(\"True Label\")\n",
    "\n",
    "plt.suptitle(\" Realistic Confusion Matrices (Diagonal Dominant)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6981579,
     "sourceId": 11184460,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
